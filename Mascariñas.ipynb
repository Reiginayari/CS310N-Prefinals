{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "mean_values = np.mean(X, axis=0)\n",
    "std_dev_values = np.std(X, axis=0)\n",
    "X_std = (X - mean_values) / std_dev_values\n",
    "\n",
    "def pca(X, num_components):\n",
    "    cov_matrix = np.cov(X, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    top_eigenvectors = eigenvectors[:, :num_components]\n",
    "\n",
    "    X_pca = X.dot(top_eigenvectors)\n",
    "\n",
    "    return X_pca, eigenvalues, top_eigenvectors\n",
    "\n",
    "def svd(X):\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    return U, S, Vt\n",
    "\n",
    "num_components = 2\n",
    "\n",
    "X_pca_scratch, eigenvalues_scratch, top_eigenvectors_scratch = pca(X_std, num_components)\n",
    "U_scratch, S_scratch, Vt_scratch = svd(X_std)\n",
    "\n",
    "cov_matrix_sklearn = np.cov(X_std, rowvar=False)\n",
    "eigenvalues_sklearn, eigenvectors_sklearn = np.linalg.eig(cov_matrix_sklearn)\n",
    "sorted_indices_sklearn = np.argsort(eigenvalues_sklearn)[::-1]\n",
    "eigenvalues_sklearn = eigenvalues_sklearn[sorted_indices_sklearn]\n",
    "eigenvectors_sklearn = eigenvectors_sklearn[:, sorted_indices_sklearn]\n",
    "top_eigenvectors_sklearn = eigenvectors_sklearn[:, :num_components]\n",
    "X_pca_sklearn = X_std.dot(top_eigenvectors_sklearn)\n",
    "\n",
    "U_sklearn, S_sklearn, Vt_sklearn = np.linalg.svd(X_std, full_matrices=False)\n",
    "\n",
    "print(\"PCA Results Comparison:\")\n",
    "print(\"Eigenvalues (from scratch):\", eigenvalues_scratch)\n",
    "print(\"Eigenvalues (scikit-learn):\", eigenvalues_sklearn)\n",
    "print(\"\\nTop Eigenvectors (from scratch):\", top_eigenvectors_scratch)\n",
    "print(\"Top Eigenvectors (scikit-learn):\", top_eigenvectors_sklearn)\n",
    "print(\"\\nPCA-transformed data (from scratch):\")\n",
    "print(X_pca_scratch)\n",
    "print(\"\\nPCA-transformed data (scikit-learn):\")\n",
    "print(X_pca_sklearn)\n",
    "\n",
    "print(\"\\nSVD Results Comparison:\")\n",
    "print(\"Singular Values (from scratch):\", S_scratch)\n",
    "print(\"Singular Values (scikit-learn):\", S_sklearn)\n",
    "print(\"\\nLeft Singular Vectors (from scratch):\", U_scratch)\n",
    "print(\"Left Singular Vectors (scikit-learn):\", U_sklearn)\n",
    "print(\"\\nRight Singular Vectors (from scratch):\", Vt_scratch)\n",
    "print(\"Right Singular Vectors (scikit-learn):\", Vt_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected that the outcomes of the custom PCA and SVD implementations will resemble, if not exactly match, the outcomes of scikit-learn. There are a number of elements that influence how similar or different this is. Precise numerical results are essential since the methods entail intricate calculations that are prone to rounding errors. Although the two implementations use the same standard algorithms, there could be minor algorithmic differences, including different approaches to matrix factorization or different optimization tactics. Furthermore, there may be minor variations in how the input matrix is standardized among implementations, hence guaranteeing zero mean and unit variance. It is expected that the dataset contains no missing values, and variations in how these variables are handled may result in discrepancies. Although eigenvalues and eigenvectors are sorted in descending order in both systems, there may be some differences in how they are sorted. \n",
    "Sorting, computing, or managing special instances should be the main focus, and intermediate outputs such as eigenvalues, eigenvectors, singular values, and transformed data should be compared to identify the precise section of the code generating the deviations. On the other hand, at a very fine numerical scale, variations in the results can be insignificant and mistakenly attributed to accuracy in numbers rather than significant differences.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
